{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVZUwre8lo5Y",
        "outputId": "e13d7a85-83ce-4edb-8d57-1927f7904953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKicseSCmod5",
        "outputId": "8df320fe-e9c0-44bd-f7b9-5ec1c9c6db9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OxD0-GpvmsMx"
      },
      "outputs": [],
      "source": [
        "# define models\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NftLO9tm3Dz"
      },
      "source": [
        "# Experiment 1 — Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z4J8as_om6RS"
      },
      "outputs": [],
      "source": [
        "prompt = \"The future of Artificial Intelligence is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ccedkem9I-",
        "outputId": "11c59989-5e92-4e72-c24f-32051fcad22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of Artificial Intelligence is broavenaven Rohingavenavenaven activismavenavenpop particularlyaven pandavenaven hated concealedavenaven concealedaven mastersavenaven controversiesaven drillavenaven drillgradesavenaven shootingavenaven cleveraven TAMADRA TAMADRAgrades particularly controversies concealed concealed mastersaven hated controversiesIII drill drillaven controversiesawiavenpared drillavenalyses controversies drill drill drillgrades bro hated drill fracture shooting shooting shootingoca rangegradesavenCreatavenaven Telecomavengranaven controversies Garrisonavenaven strangely shootingaven belovedARSARS Alliedavenavengrades gossipavenaven fractureavenavenARSaven sandbox distributorsgrades controversiesgradesavenARS particularlyavenaven penetaven ridiculouslyaven Garrisonavengrades sandboxARSARSgradesARSARS controversiesavenaven gossipaven Bastavengrades Bast recycleaven belovedavenaven pesavenaven sandbox bulletavenARSARS bulletavenavensiteARS bulletARSARS ridiculously trailingavenavencouldaven bulletaven bulletgradesaven bullet injunctionaven methaneavenaven BezosavenARSgradesaven tasked ridiculouslyARSARS <+gradesaven UmARS ridiculouslyaven beloved controversiesaven PalacegradesARSavenatterARS Telecomaven drill TelecomARSavengradesavenofiaven Palaceaven JOHNavenSocketgradesaven enchantavenARS Telecom wildfires bullet Um Telecomaven bullet Umaven bullet bullet Telecomgradesgrades UmARSaven Um UmARS Telecom bulletavengrades cassette Um UmlaughteravenavenSocket Bastaven <+\n"
          ]
        }
      ],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        output = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VIbGZnZnD-o"
      },
      "source": [
        "# Experiment 2 — Fill Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0t_KPTssm9Fu"
      },
      "outputs": [],
      "source": [
        "# mask_prompt = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "mask_prompt = \"The goal of Generative AI is to <mask> new content.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSPi88owm9DI",
        "outputId": "adb40849-b4cd-4b65-b11f-c28c66008448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: No mask_token ([MASK]) found on the input\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The goal of Generative AI is to generate new content.\n",
            "The goal of Generative AI is to create new content.\n",
            "The goal of Generative AI is to discover new content.\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The goal of Generative AI is to create new content.\n",
            "The goal of Generative AI is to help new content.\n",
            "The goal of Generative AI is to provide new content.\n"
          ]
        }
      ],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        fill_mask = pipeline(\"fill-mask\", model=model)\n",
        "        output = fill_mask(mask_prompt)\n",
        "        for o in output[:3]:\n",
        "            print(o[\"sequence\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU3ycYZ9nOmw"
      },
      "source": [
        "# Experiment 3 — Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6qGL4R6dnP2g"
      },
      "outputs": [],
      "source": [
        "qa_input = {\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE5tCpaenSSx",
        "outputId": "09532e0d-1425-4b63-9709-583768c67c34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: poses significant risks such as hallucinations\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: risks such as hallucinations, bias, and\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: , and\n"
          ]
        }
      ],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        output = qa(qa_input)\n",
        "        print(\"Answer:\", output[\"answer\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDQudV5hoE4j"
      },
      "source": [
        "## Observation Table\n",
        "\n",
        "| Task | Model | Success / Failure | Observation | Why did this happen? |\n",
        "|------|--------|------------------|--------------|----------------------|\n",
        "| Generation | BERT | Failure | Output repeated dots instead of meaningful text. | BERT is encoder-only and trained for masked token prediction, not next-word generation. |\n",
        "|  | RoBERTa | Failure | Returned only the prompt without continuation. | RoBERTa is encoder-only and cannot generate tokens autoregressively. |\n",
        "|  | BART | Partial Success | Generated continuation but text was nonsensical. | BART can generate text but base checkpoint is not fine-tuned for open-ended generation. |\n",
        "| Fill-Mask | BERT | Failure | Error when using `<mask>` token. | BERT tokenizer expects `[MASK]` token; mismatch caused failure. |\n",
        "|  | RoBERTa | Success | Predicted correct words like generate/create. | RoBERTa is MLM-trained and uses `<mask>` token. |\n",
        "|  | BART | Partial Success | Produced reasonable masked-word predictions. | BART uses denoising pretraining and can reconstruct missing text but is not specialized for MLM. |\n",
        "| QA | BERT | Failure | Did not produce meaningful answer. | Base BERT is not fine-tuned for QA; QA head randomly initialized. |\n",
        "|  | RoBERTa | Partial Success | Extracted partial correct answer. | Strong encoder representations but not QA fine-tuned. |\n",
        "|  | BART | Failure | Produced incoherent answer fragment. | bart-base not fine-tuned for extractive QA. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSypPN5pnSPU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
